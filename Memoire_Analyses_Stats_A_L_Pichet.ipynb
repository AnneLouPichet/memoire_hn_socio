{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mémoire d'Anne-Lou Pichet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des bibliothèques et fonctions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "#!python -m spacy download fr_core_news_lg\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listes de noms par type de carrière\n",
    "carriere_loisir_socio = ['Agathe','Alya', 'Jules', 'Salomé', 'Alice ']\n",
    "carriere_semi_professionnelle = ['Clément', 'Claire ', 'Amélie', 'Capucine', 'Hélène', 'Christine ', 'Laureline', 'Elisabeth', 'Jade']\n",
    "carriere_milieu_socio = ['Aline', 'Arthur', 'Emma', 'Ariane', 'Maude']\n",
    "# Fonction pour déterminer le type de carrière\n",
    "def get_type_carriere(nom):\n",
    "    if nom in carriere_loisir_socio:\n",
    "        return 'loisir'\n",
    "    elif nom in carriere_semi_professionnelle:\n",
    "        return 'semi_professionnelle'\n",
    "    elif nom in carriere_milieu_socio:\n",
    "        return 'milieu'\n",
    "    else:\n",
    "        return 'inconnu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire et classifier les hashtags\n",
    "def analyser_hashtags(post):\n",
    "    hashtags = re.findall(r\"#(\\w+)\", str(post))\n",
    "    total_hashtags = len(hashtags)\n",
    "    return {\n",
    "        \"Contient_Hashtags\": total_hashtags > 0,\n",
    "        \"Nombre_Hashtags\": total_hashtags,\n",
    "        \"Liste_Hashtags\": hashtags\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire et classifier les hashtags dans les posts (commençant par un #)\n",
    "def analyser_hashtags(post):\n",
    "    hashtags = re.findall(r\"#(\\w+)\", str(post))\n",
    "    total_hashtags = len(hashtags)\n",
    "    return {\n",
    "        \"Contient_Hashtags\": total_hashtags > 0,\n",
    "        \"Nombre_Hashtags\": total_hashtags,  \n",
    "        \"Liste_Hashtags\": hashtags,  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire et classifier les mentions dans les posts (commençant par un @)\n",
    "def analyser_mentions(post):\n",
    "    mentions = re.findall(r\"@(\\w+)\", str(post))\n",
    "    total_mentions = len(mentions)\n",
    "    mentions_auteurs = [m for m in mentions if m.startswith(\"A\")]  # Mentions d'auteurs\n",
    "    mentions_maisons = [m for m in mentions if m.startswith(\"ME\")]  # Mentions de maisons d'édition\n",
    "    mentions_pairs = [m for m in mentions if m.startswith(\"Pa\")]  # Mentions de pairs\n",
    "    mentions_autres = [m for m in mentions if not (m.startswith(\"A\") or m.startswith(\"ME\") or m.startswith(\"Pa\"))]  # Mentions autres\n",
    "    return {\n",
    "        \"Contient_Mentions\": total_mentions > 0,  \n",
    "        \"Nombre_Mentions\": total_mentions,  \n",
    "        \"Mentions_Auteurs\": len(mentions_auteurs),  \n",
    "        \"Mentions_Maisons\": len(mentions_maisons),  \n",
    "        \"Mentions_Autres\": len(mentions_autres),  \n",
    "        \"Mentions_Pairs\": len(mentions_pairs),  \n",
    "        \"Contient_Mentions_Pairs\": len(mentions_pairs) > 0,  \n",
    "        \"Contient_Mentions_Auteurs\": len(mentions_auteurs) > 0,\n",
    "        \"Contient_Mentions_Autres\": len(mentions_autres) > 0,\n",
    "        \"Contient_Mentions_Maisons\": len(mentions_maisons) > 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données et prétraitements de celles-ci  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code a pour but de transformer la structure de mes données pour pouvoir, par la suite, construire des statistiques à leur propos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_brutes = pd.read_excel(\"Posts_Anon.xlsx\") # fichier d'origine des données à transformer \n",
    "images_brutes = pd.read_excel(\"Images_Posts.xlsx\", engine='openpyxl') # fichier contenant les données des images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation des données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les données transformées\n",
    "format_propre = []\n",
    "\n",
    "# Pour chaque auteur dans le DataFrame, on parcourt les colonnes et on remet en forme les données\n",
    "for _, row in data_brutes.iterrows():\n",
    "    for col in data_brutes.columns:\n",
    "        match = re.match(r\"Contenu_([A-Za-z0-9]+)_Post\", col)  # On extrait le code du post (ex: F1, L2, A1E3...) \n",
    "        if match:  \n",
    "            code_post = match.group(1)  # On récupère le code du post\n",
    "            \n",
    "            # Récupérer les valeurs associées aux autres attributs\n",
    "            contenu = row[f\"Contenu_{code_post}_Post\"]\n",
    "            type_post = row.get(f\"Type_{code_post}_Post\", None)\n",
    "            annexes = row.get(f\"Annexes_{code_post}_Post\", None)\n",
    "            date_post = row.get(f\"Date_{code_post}_Post\", None)\n",
    "            \n",
    "            code_propre = None\n",
    "            nombre_propre = None\n",
    "            code_match = re.match(r\"^(.+?)(?=\\d+$)\", code_post)  # Tout sauf les derniers chiffres\n",
    "            nombre_match = re.search(r\"(\\d+)$\", code_post)       # Les derniers chiffres uniquement\n",
    "            code_propre = code_match.group(1)\n",
    "            nombre_propre = nombre_match.group(1)\n",
    "            nombre_propre = float(nombre_propre)\n",
    "\n",
    "            # Si un contenu ou une date existe, ajouter une nouvelle ligne\n",
    "            if pd.notna(date_post) or pd.notna(contenu):\n",
    "                format_propre.append({\n",
    "                    \"Nom_d_emprunt\": row[\"Nom d'emprunt\"],\n",
    "                    \"Contenu_Post\": contenu,\n",
    "                    \"Type_Post\": type_post,\n",
    "                    \"Code_Post\": code_post, \n",
    "                    \"Code\": code_propre,\n",
    "                    \"Nombre\": nombre_propre,\n",
    "                    \"Annexes_Post\": annexes,\n",
    "                    \"Date_Post\": date_post\n",
    "                })\n",
    "             \n",
    "# Créer un DataFrame final\n",
    "data_propre = pd.DataFrame(format_propre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de correspondance pour corriger les erreurs\n",
    "corrections = {\n",
    "    \"Vidéos\": \"Vidéo\",\n",
    "    \"Vdéo\": \"Vidéo\",\n",
    "    \"Videos\": \"Vidéo\",\n",
    "    \"DIvers\": \"Divers\",\n",
    "    \"Dievrs\": \"Divers\",\n",
    "    \"Dvers\": \"Divers\",\n",
    "    \"Recap\": \"Récap\",\n",
    "    \"chronique\": \"Chronique\",\n",
    "    \"recommandations\": \"Recommandations\",\n",
    "    \"Recommendations\": \"Recommandations\",\n",
    "    \"Auteur\": \"Professionnel\",  # Si \"Auteur\" désigne un post pro\n",
    "    \"Extrait\": \"Extrait\",\n",
    "    \"Citation\": \"Citation\",\n",
    "    \"Podcast\": \"Audio\",\n",
    "    \"Haul\": \"Haul\",\n",
    "    \"Challenge\": \"Challenge\",\n",
    "    \"Présentation\": \"Présentation\",\n",
    "    \"Bilan\": \"Bilan\",\n",
    "    \"Récap\": \"Récap\",\n",
    "}\n",
    "\n",
    "# Supprimer les valeurs clairement incorrectes\n",
    "valeurs_a_supprimer = {'', 'p', ']', 'www.instagram.com', 'https:', 'CthTGTTIj7u'}\n",
    "\n",
    "# Fonction pour nettoyer les types de post\n",
    "def clean_type_post(t):\n",
    "    if pd.isna(t):\n",
    "        return None\n",
    "    cleaned_values = {corrections.get(x.strip(), x.strip()) for x in t.split('/')}\n",
    "    cleaned_values.difference_update(valeurs_a_supprimer)  \n",
    "    cleaned_values = {re.sub(r'\\[.*?\\]|https?:\\/\\/\\S+', '', x) for x in cleaned_values}\n",
    "    cleaned_values.discard('')\n",
    "    return '/'.join(sorted(cleaned_values)) if cleaned_values else None  \n",
    "\n",
    "# Appliquer le nettoyage\n",
    "data_propre['Type_Post'] = data_propre['Type_Post'].apply(clean_type_post)\n",
    "\n",
    "# Afficher les nouvelles valeurs uniques\n",
    "print(data_propre['Type_Post'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déterminer la nature des posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_post_type(row):\n",
    "    type_post = row['Type_Post']\n",
    "    \n",
    "    if pd.isna(type_post):\n",
    "        return None  # Aucun type si NaN\n",
    "    \n",
    "    types = set(type_post.split('/'))  # Séparer les types multiples\n",
    "\n",
    "    # On garde toutes les catégories valides\n",
    "    categories = set()\n",
    "    if 'Vidéo' in types:\n",
    "        categories.add('Vidéo')\n",
    "    if 'Audio' in types:\n",
    "        categories.add('Audio')\n",
    "    if pd.notna(row['Contenu_Post']):\n",
    "        categories.add('Écrit')\n",
    "\n",
    "    return '/'.join(sorted(categories)) if categories else None  # Trier pour homogénéité\n",
    "\n",
    "# Appliquer la fonction mise à jour\n",
    "data_propre['Type_Post'] = data_propre.apply(determine_post_type, axis=1)\n",
    "\n",
    "# Créer les colonnes binaires\n",
    "data_propre['N_Video'] = data_propre['Type_Post'].str.contains('Vidéo', na=False).astype(int)\n",
    "data_propre['N_Audio'] = data_propre['Type_Post'].str.contains('Audio', na=False).astype(int)\n",
    "data_propre['N_Ecrit'] = data_propre['Type_Post'].str.contains('Écrit', na=False).astype(int)\n",
    "\n",
    "data_propre.drop(columns=['Type_Post'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge avec les données des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout d'un index commun\n",
    "data_propre['index'] = data_propre.groupby(['Nom_d_emprunt', 'Date_Post']).cumcount()\n",
    "images_brutes['index'] = images_brutes.groupby(['Nom_d_emprunt', 'Date_Post']).cumcount()\n",
    "\n",
    "# conversion des colonnes spécifiques en numérique\n",
    "images_brutes[['Plusieurs_Images', 'Personne', 'PostProduction', 'Mots']] = images_brutes[['Plusieurs_Images', 'Personne', 'PostProduction', 'Mots']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fusion des données\n",
    "base_complete = pd.merge(data_propre, images_brutes, on=['Nom_d_emprunt', 'Date_Post', 'index'], how='left')\n",
    "\n",
    "\n",
    "# Save the merged_data to an Excel file\n",
    "#base_complete.to_excel(\"Base_Posts_Anon.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données traitées pour les statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"Base_Posts_Anon.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion des périodes et des identifiants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On tranforme les périodes pour qu'elles soient plus lisibles\n",
    "data[\"Date_Post\"] = pd.to_datetime(data[\"Date_Post\"], errors=\"coerce\")\n",
    "data[\"Periode\"] = data[\"Code\"].str.extract(r\"^([A-Za-z]+[0-9]*)\") \n",
    "\n",
    "recodage_periodes = {\"F\": 1, \"A1\": 2, \"A2\": 3, \"L\": 4} # Dictionnaire de recodage des périodes\n",
    "# On gére les posts épinglés\n",
    "data[\"Periode\"] = data[\"Periode\"].apply(lambda x: recodage_periodes.get(x, \"E\"))\n",
    "\n",
    "# On ajoute un Identifiant pour chaque post\n",
    "data[\"ID_Post\"] = range(1, len(data) + 1)\n",
    "data.set_index(\"ID_Post\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout des types de carrières "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['type_carriere'] = data['Nom_d_emprunt'].apply(get_type_carriere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réattribuer les posts épinglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Date_Post\"] = pd.to_datetime(data[\"Date_Post\"], errors=\"coerce\")\n",
    "data[\"Période_Bis\"] = data[\"Periode\"] \n",
    "data = data.sort_values(by=[\"Nom_d_emprunt\", \"Periode\", \"Date_Post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réaffectation des posts \"E\"\n",
    "for auteur in data[\"Nom_d_emprunt\"].unique():\n",
    "    auteur_data = data[data[\"Nom_d_emprunt\"] == auteur].copy()\n",
    "\n",
    "    posts_E = auteur_data[auteur_data[\"Periode\"] == \"E\"]\n",
    "    autres_periodes = auteur_data[auteur_data[\"Periode\"] != \"E\"]\n",
    "\n",
    "    for periode in autres_periodes[\"Periode\"].unique():\n",
    "        periode_data = autres_periodes[autres_periodes[\"Periode\"] == periode]\n",
    "        if periode_data[\"Date_Post\"].isnull().all():\n",
    "            continue\n",
    "        min_date = periode_data[\"Date_Post\"].min()\n",
    "        max_date = periode_data[\"Date_Post\"].max()\n",
    "\n",
    "        for idx_E, row_E in posts_E.iterrows():\n",
    "            date_E = row_E[\"Date_Post\"]\n",
    "            if pd.notnull(date_E) and min_date <= date_E <= max_date:\n",
    "                data.at[idx_E, \"Période_Bis\"] = periode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du tableau des plages par période\n",
    "plages_periode = []\n",
    "\n",
    "for auteur in data[\"Nom_d_emprunt\"].unique():\n",
    "    auteur_data = data[(data[\"Nom_d_emprunt\"] == auteur) & (data[\"Periode\"] != \"E\")]\n",
    "\n",
    "    for periode in auteur_data[\"Periode\"].unique():\n",
    "        periode_data = auteur_data[auteur_data[\"Periode\"] == periode]\n",
    "        if periode_data[\"Date_Post\"].isnull().all():\n",
    "            continue\n",
    "\n",
    "        plages_periode.append({\n",
    "            \"Auteur\": auteur,\n",
    "            \"Periode\": periode,\n",
    "            \"Date_Début\": periode_data[\"Date_Post\"].min(),\n",
    "            \"Date_Fin\": periode_data[\"Date_Post\"].max()\n",
    "        })\n",
    "\n",
    "plages = pd.DataFrame(plages_periode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des rattachements des posts épinglés (\"E\")\n",
    "posts_E_originaux = data[data[\"Periode\"] == \"E\"]\n",
    "\n",
    "verifications = []\n",
    "\n",
    "for idx, row in posts_E_originaux.iterrows():\n",
    "    auteur = row[\"Nom_d_emprunt\"]\n",
    "    date_post = row[\"Date_Post\"]\n",
    "    periode_bis = row[\"Période_Bis\"]\n",
    "\n",
    "    valide = False\n",
    "\n",
    "    auteur_data = data[(data[\"Nom_d_emprunt\"] == auteur) & (data[\"Periode\"] != \"E\")]\n",
    "    for periode in auteur_data[\"Periode\"].unique():\n",
    "        periode_data = auteur_data[auteur_data[\"Periode\"] == periode]\n",
    "        if periode_data[\"Date_Post\"].isnull().all():\n",
    "            continue\n",
    "        min_date = periode_data[\"Date_Post\"].min()\n",
    "        max_date = periode_data[\"Date_Post\"].max()\n",
    "\n",
    "        if pd.notnull(date_post) and min_date <= date_post <= max_date:\n",
    "            valide = (periode_bis == periode)\n",
    "            break\n",
    "\n",
    "    verifications.append({\n",
    "        \"Index\": idx,\n",
    "        \"Auteur\": auteur,\n",
    "        \"Date_Post\": date_post,\n",
    "        \"Période_Bis\": periode_bis,\n",
    "        \"Date_dans_une_plage_connue\": valide\n",
    "    })\n",
    "\n",
    "verifications = pd.DataFrame(verifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie les dates des posts épinglés par rapport aux plages connues\n",
    "verifs = verifications.merge(plages, how=\"left\",\n",
    "                                  left_on=[\"Auteur\", \"Période_Bis\"],\n",
    "                                  right_on=[\"Auteur\", \"Periode\"])\n",
    "\n",
    "verifs = verifs[[\n",
    "    \"Index\", \"Auteur\", \"Date_Post\", \"Période_Bis\", \"Date_dans_une_plage_connue\",\n",
    "    \"Date_Début\", \"Date_Fin\"\n",
    "]].sort_values(by=[\"Auteur\", \"Date_Post\"])\n",
    "\n",
    "\n",
    "print(\"Nombre de posts 'E' au départ :\", len(posts_E_originaux))\n",
    "print(\"Nombre de posts 'E' encore non rattachés :\", (data[\"Période_Bis\"] == \"E\").sum())\n",
    "print(\"\\nRattachements incorrects ou non attribués :\")\n",
    "verifs[verifs[\"Date_dans_une_plage_connue\"] == False] # Pour voir si il y a des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des périodes pour certains enquêtés pour vérifier les rattachements à la main \n",
    "enquetes = ['Amélie', 'Capucine', 'Christine', 'Claire', 'Emma', 'Jade', 'Laureline', 'Salomé']\n",
    "\n",
    "for auteur in data[\"Nom_d_emprunt\"].unique():\n",
    "    if auteur in enquetes:\n",
    "        auteur_data = data[(data[\"Nom_d_emprunt\"] == auteur) & (data[\"Periode\"] != \"E\")]\n",
    "        print(f\"\\nPériodes pour l'auteur {auteur} :\")\n",
    "        for periode in auteur_data[\"Periode\"].unique():\n",
    "            periode_data = auteur_data[auteur_data[\"Periode\"] == periode]\n",
    "            if not periode_data[\"Date_Post\"].isnull().all():\n",
    "                min_date = periode_data[\"Date_Post\"].min()\n",
    "                max_date = periode_data[\"Date_Post\"].max()\n",
    "                print(f\"Période {periode}: Début: {min_date}, Fin: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"Base_Posts_Anon.xlsx\") # Pour sauvegarder les modifications la première fois que le script est exécuté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des indicateurs statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Médiane des posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenir le nombre total et la médiane de posts par enquêté\n",
    "posts_par_enquete = data.groupby('Nom_d_emprunt').size().reset_index(name='Nombre de Posts')\n",
    "mediane_posts = posts_par_enquete['Nombre de Posts'].median()\n",
    "mediane_posts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréquence de posts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On analyse ici la fréquence à laquelle nos enquêtés postent sur Instagram à propos de leurs lectures. \n",
    "Pour cela, nous calculons la fréquence entre chaque posts d'une même période et pour un même enquêté. Nous croiserons par la suite les résulats avec plusieurs autres indicateurs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_freq = data.sort_values(by=[\"Nom_d_emprunt\", \"Période_Bis\", \"Date_Post\"])\n",
    "\n",
    "frequence = []\n",
    "\n",
    "for auteur in data_freq[\"Nom_d_emprunt\"].unique():\n",
    "    auteur_data = data_freq[data_freq[\"Nom_d_emprunt\"] == auteur]\n",
    "    carriere = auteur_data[\"type_carriere\"].unique()\n",
    "    for periode in auteur_data[\"Période_Bis\"].unique():\n",
    "        periode_data = auteur_data[auteur_data[\"Période_Bis\"] == periode]\n",
    "        \n",
    "        if periode_data[\"Date_Post\"].isnull().all():\n",
    "            frequence.append({\n",
    "                \"Nom_d_emprunt\": auteur,\n",
    "                \"Période_Bis\": periode,\n",
    "                \"Carrière\": carriere,\n",
    "                \"Nombre_de_Posts\": 0,\n",
    "                \"Intervalle_Moyen\": None,\n",
    "                \"Intervalle_Minimum\": None,\n",
    "                \"Intervalle_Maximum\": None,\n",
    "                \"Ecart_Type_Intervalle\": None\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        nombre_de_posts = len(periode_data)\n",
    "        \n",
    "        periode_data = periode_data.sort_values(by=\"Date_Post\")\n",
    "        periode_data[\"Intervalle_Posts\"] = periode_data[\"Date_Post\"].diff().dt.days\n",
    "        \n",
    "        periode_data = periode_data.dropna(subset=[\"Intervalle_Posts\"])\n",
    "        \n",
    "        if not periode_data.empty:\n",
    "            frequence.append({\n",
    "                \"Nom_d_emprunt\": auteur,\n",
    "                \"Période_Bis\": periode,\n",
    "                \"Carrière\": carriere,\n",
    "                \"Nombre_de_Posts\": nombre_de_posts, \n",
    "                \"Intervalle_Moyen\": periode_data[\"Intervalle_Posts\"].mean(),\n",
    "                \"Intervalle_Minimum\": periode_data[\"Intervalle_Posts\"].min(),\n",
    "                \"Intervalle_Maximum\": periode_data[\"Intervalle_Posts\"].max(),\n",
    "                \"Intervalle_Mediane\": periode_data[\"Intervalle_Posts\"].median(),\n",
    "                \"Ecart_Type_Intervalle\": periode_data[\"Intervalle_Posts\"].std()\n",
    "            })\n",
    "        else:\n",
    "            # Si les intervalles ne peuvent pas être calculés, on ajoute des valeurs par défaut\n",
    "            frequence.append({\n",
    "                \"Nom_d_emprunt\": auteur,\n",
    "                \"Période_Bis\": periode,\n",
    "                \"Carrière\": carriere,\n",
    "                \"Nombre_de_Posts\": nombre_de_posts,\n",
    "                \"Intervalle_Moyen\": None,\n",
    "                \"Intervalle_Minimum\": None,\n",
    "                \"Intervalle_Maximum\": None,\n",
    "                \"Intervalle_Mediane\": None,\n",
    "                \"Ecart_Type_Intervalle\": None\n",
    "            })\n",
    "\n",
    "frequence = pd.DataFrame(frequence)\n",
    "frequence.rename(columns={\"Période_Bis\": \"Periode\",}, inplace=True)\n",
    "frequence.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des fréquences selon les carrières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On croise l'analyse des fréquences de posts avec les carrières. \n",
    "Comme nous avons un outlier impactant fortement les données des fréquences (Capucine), nous avons voulu voir la différence en l'enlevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_carriere = []\n",
    "\n",
    "frequence[\"Carrière\"] =frequence[\"Carrière\"].apply(lambda x: str(x))\n",
    "for carriere in frequence[\"Carrière\"].unique():\n",
    "    carriere_data = frequence[frequence[\"Carrière\"] == carriere]\n",
    "\n",
    "    # Suppression de l'outlier pour voir son effet sur la moyenne\n",
    "    max_val = carriere_data['Intervalle_Moyen'].max()\n",
    "    carriere_data_sans_outlier = carriere_data[carriere_data['Intervalle_Moyen'] != max_val]\n",
    "\n",
    "    freq_carriere.append({\n",
    "        \"Carrière\": carriere,\n",
    "        \"Nombre de posts\": carriere_data['Nombre_de_Posts'].sum(),\n",
    "        \"Intervalle minimum\": carriere_data['Intervalle_Minimum'].min(),\n",
    "        \"Intervalle maximum\": carriere_data['Intervalle_Maximum'].max(),\n",
    "        \"Intervalle moyen\": carriere_data['Intervalle_Moyen'].mean(),\n",
    "        \"Intervalle médian\": carriere_data['Intervalle_Mediane'].median(),\n",
    "        \"Intervalle moyen (sans outlier)\": carriere_data_sans_outlier['Intervalle_Moyen'].mean()\n",
    "    })\n",
    "\n",
    "freq_carriere = pd.DataFrame(freq_carriere)\n",
    "freq_carriere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant chercher les statistiques de fréquences croisées avec les carrières et les périodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequence.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_carriere_periode = []\n",
    "for carriere in frequence[\"Carrière\"].unique():\n",
    "    carriere_data = frequence[frequence[\"Carrière\"] == carriere]\n",
    "\n",
    "    # Boucle sur chaque période sauf les posts épinglés ('E')\n",
    "    for periode in carriere_data[\"Periode\"].unique():\n",
    "        if periode == 'E':\n",
    "            continue\n",
    "        periode_data = carriere_data[carriere_data[\"Periode\"] == periode]\n",
    "\n",
    "        nb_posts = periode_data[\"Nombre_de_Posts\"].sum()\n",
    "        intervalle_min = periode_data[\"Intervalle_Minimum\"].min()\n",
    "        intervalle_max = periode_data[\"Intervalle_Maximum\"].max()\n",
    "        intervalle_moy = periode_data[\"Intervalle_Moyen\"].mean()\n",
    "        intervalle_med = periode_data[\"Intervalle_Mediane\"].median()\n",
    "\n",
    "        freq_carriere_periode.append({\n",
    "            \"Carrière\": carriere,\n",
    "            \"Période\": periode,\n",
    "            \"Nombre de posts\": nb_posts,\n",
    "            \"Intervalle minimum\": intervalle_min,\n",
    "            \"Intervalle maximum\": intervalle_max,\n",
    "            \"Intervalle moyen\": round(intervalle_moy, 2), \n",
    "            \"Intervalle médian\": round(intervalle_med, 2),\n",
    "        })\n",
    "\n",
    "freq_carriere_periode = pd.DataFrame(freq_carriere_periode)\n",
    "\n",
    "freq_carriere_periode.sort_values(by=[\"Carrière\", \"Période\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selon les périodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ici croiser les statistiques de fréquence avec les périodes, pour identifier de potentielles variations dans le temps. Pour cela, on inclut des pondérations car tout les enquêtés n'ont pas le meme nombre de posts pour chaque période. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequence_periode = pd.DataFrame(columns=[\"Période\"])\n",
    "for periode_bis in frequence[\"Période_Bis\"].unique():\n",
    "    periode_data = frequence[frequence[\"Période_Bis\"] == periode_bis]\n",
    "    # intervalle moyen\n",
    "    intervalle_moyen_pondere = (periode_data[\"Intervalle_Moyen\"] * periode_data[\"Nombre_de_Posts\"]).sum() / periode_data[\"Nombre_de_Posts\"].sum()\n",
    "    # Intervalle minimum\n",
    "    intervalle_minimum = periode_data[\"Intervalle_Minimum\"].min()\n",
    "    # Intervalle maximum\n",
    "    intervalle_maximum = periode_data[\"Intervalle_Maximum\"].max()\n",
    "    # Intervalle median\n",
    "    mediane_intervalle = periode_data[\"Intervalle_Moyen\"].median()\n",
    "    #ecart type pondéré\n",
    "    ecart_type_pondere = (periode_data[\"Ecart_Type_Intervalle\"] * periode_data[\"Nombre_de_Posts\"]).sum() / periode_data[\"Nombre_de_Posts\"].sum()\n",
    "    # Nombre total de posts dans cette période\n",
    "    nombre_posts = periode_data[\"Nombre_de_Posts\"].sum()\n",
    "\n",
    "    # Ajouter les résultats au tableau récapitulatif\n",
    "    nouvelle_ligne = pd.DataFrame([{\n",
    "        \"Période\": periode_bis,\n",
    "       \"Intervalle_Moyen\": intervalle_moyen_pondere,\n",
    "       \"Médiane_Intervalle\": mediane_intervalle,\n",
    "        \"Intervalle_Minimum\": intervalle_minimum,\n",
    "        \"Intervalle_Maximum\": intervalle_maximum,\n",
    "        \"Ecart_Type\": ecart_type_pondere,\n",
    "        \"Intervalle_Mediane\": mediane_intervalle,\n",
    "        \"Nombre_de_Posts\": nombre_posts,\n",
    "\n",
    "    }])\n",
    "\n",
    "    frequence_periode = pd.concat([frequence_periode, nouvelle_ligne], ignore_index=True)\n",
    "\n",
    "# Ajout des données pour toutes les périodes \n",
    "\n",
    "ligne_total = pd.DataFrame([{\n",
    "    \"Période\": \"Total\",\n",
    "    \"Intervalle_Moyen\": frequence_periode[\"Intervalle_Moyen\"].mean(),\n",
    "    \"Médiane_Intervalle\": frequence_periode[\"Médiane_Intervalle\"].median(),\n",
    "    \"Intervalle_Minimum\": frequence_periode[\"Intervalle_Minimum\"].min(),\n",
    "    \"Intervalle_Maximum\": frequence_periode[\"Intervalle_Maximum\"].max(),\n",
    "    \"Ecart_Type\": frequence_periode[\"Ecart_Type\"].mean(),\n",
    "    \"Nombre_de_Posts\": frequence_periode[\"Nombre_de_Posts\"].sum(),\n",
    "}])\n",
    "\n",
    "frequence_periode = pd.concat([frequence_periode, ligne_total], ignore_index=True)\n",
    "\n",
    "frequence_periode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longueur des posts et des annexes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ici analyser le contenu des posts par leur longeur (à la fois le corps des posts mais aussi leurs éventuelles annexes). \n",
    "\n",
    "Nous avons pour cette analyse, tenté plusieurs choses : \n",
    "- un calcul de la longeur en mots et en caractères. On a finalement retenu ce dernier indicateurs pour nos analyses. \n",
    "- nous avons envisagé de faire une analyse plus poussée sur le langage utilisée dans les posts de nos enquêtés, avant de plutôt nous tourner vers des clusterings pour identifier des clusters de pratiques et d'enquêtés. Dans un travail suivant, il serait alors pertinent de poursuivre par une analyse TAL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la longueur des posts (en caractères et mots)\n",
    "data[\"Contenu_Post\"] = data[\"Contenu_Post\"].astype(str)  # Gérer les NaN\n",
    "data[\"Longueur_Mots\"] = data[\"Contenu_Post\"].apply(lambda x: len(x.split()))  # Longueur en mots\n",
    "data[\"Longueur_Contenu\"] = data[\"Contenu_Post\"].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
    "\n",
    "data[\"Presence_Annexes\"] = data[\"Annexes_Post\"].apply(lambda x: bool(pd.notna(x) and str(x).strip() and str(x).lower() != 'nan'))\n",
    "# On identifie les annexes dans les posts avec [AP]\n",
    "data[\"Annexes_Post\"] = data[\"Annexes_Post\"].astype(str)  # Convertir en chaînes de caractères\n",
    "data[\"Longueur_Annexes\"] = data[\"Annexes_Post\"].str.replace(r\"\\[AP\\]\", \"\", regex=True).apply(len)\n",
    "data[\"Nombre_Annexes\"] = data[\"Annexes_Post\"].apply(\n",
    "    lambda x: len(str(x).split(\"[AP]\")) if pd.notna(x) and str(x).strip() and str(x).lower() != 'nan' else 0\n",
    ")\n",
    "\n",
    "# Calculer la longueur totale (contenu + annexes)\n",
    "data[\"Longueur_Totale\"] = data[\"Longueur_Contenu\"] + data[\"Longueur_Annexes\"]\n",
    "\n",
    "\n",
    "# Colonnes pour les caractéristiques NLP des posts et de leurs annexes\n",
    "data[\"Entites_Nommees\"] = None\n",
    "data[\"Tags_POS\"] = None\n",
    "data[\"Vecteur_Norm\"] = None\n",
    "data[\"Entites_Nommees_Annexes\"] = None\n",
    "data[\"Tags_POS_Annexes\"] = None\n",
    "data[\"Vecteur_Norm_Annexes\"] = None\n",
    "\n",
    "# Analyse NLP sur chaque post et ses annexes\n",
    "for i, row in data.iterrows():\n",
    "    doc_post = nlp(row[\"Contenu_Post\"])\n",
    "    entites_post = [(ent.text, ent.label_) for ent in doc_post.ents]\n",
    "    pos_tags_post = [(token.text, token.pos_) for token in doc_post]\n",
    "    vecteur_norm_post = doc_post.vector_norm\n",
    "       \n",
    "    # On ajoute les résultats pour le corps du post\n",
    "    data.at[i, \"Entites_Nommees\"] = str(entites_post)\n",
    "    data.at[i, \"Tags_POS\"] = str(pos_tags_post)\n",
    "    data.at[i, \"Vecteur_Norm\"] = vecteur_norm_post\n",
    "    \n",
    "    # Puis on fait de même pour les annexes\n",
    "    annexes = row[\"Annexes_Post\"].split(\"[AP]\") if row[\"Annexes_Post\"].strip() else []\n",
    "    entites_annexes = []\n",
    "    pos_tags_annexes = []\n",
    "    vecteur_norm_annexes = 0\n",
    "    \n",
    "    if annexes:\n",
    "        for annexe in annexes:\n",
    "            annexe = annexe.strip()\n",
    "            if annexe and annexe != \"nan\": \n",
    "                doc_annexe = nlp(annexe)\n",
    "                entites_annexes.extend([(ent.text, ent.label_) for ent in doc_annexe.ents])\n",
    "                pos_tags_annexes.extend([(token.text, token.pos_) for token in doc_annexe])\n",
    "                vecteur_norm_annexes += doc_annexe.vector_norm \n",
    "    data.at[i, \"Entites_Nommees_Annexes\"] = str(entites_annexes)\n",
    "    data.at[i, \"Tags_POS_Annexes\"] = str(pos_tags_annexes)\n",
    "    data.at[i, \"Vecteur_Norm_Annexes\"] = vecteur_norm_annexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de la longeur des posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longueur totale (contenu + annexes)\n",
    "longueur_totale_data = data.dropna(subset=[\"Longueur_Totale\"])\n",
    "\n",
    "#rajouter la précense des annexes en pourcentages\n",
    "stats_globales_longueur = {\n",
    "    \"Type\": \"Global\",\n",
    "    \"Nombre_de_Posts\": len(longueur_totale_data),\n",
    "    \"Longueur_Moyenne\": longueur_totale_data[\"Longueur_Totale\"].mean(),\n",
    "    \"Longueur_Minimale\": longueur_totale_data[\"Longueur_Totale\"].min(),\n",
    "    \"Longueur_Maximale\": longueur_totale_data[\"Longueur_Totale\"].max(),\n",
    "    \"Longueur_Mediane\": longueur_totale_data[\"Longueur_Totale\"].median(),\n",
    "    \"Ecart_Type_Longueur\": longueur_totale_data[\"Longueur_Totale\"].std(),\n",
    "    \"Pourcentage_Annexes\": (longueur_totale_data[\"Presence_Annexes\"].sum() / len(longueur_totale_data)) * 100,\n",
    "    \"Nombres d'Annexes\" : longueur_totale_data[\"Nombre_Annexes\"].sum()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periode_longueur = []\n",
    "\n",
    "for periode in data[\"Periode\"].dropna().unique():\n",
    "    periode_data = data[data[\"Periode\"] == periode].dropna(subset=[\"Longueur_Totale\"])\n",
    "    \n",
    "    if not periode_data.empty:\n",
    "        periode_longueur.append({\n",
    "            \"Type\": f\"Période: {periode}\",\n",
    "            \"Nombre_de_Posts\": len(periode_data),\n",
    "            \"Longueur_Moyenne\": periode_data[\"Longueur_Totale\"].mean(),\n",
    "            \"Longueur_Minimale\": periode_data[\"Longueur_Totale\"].min(),\n",
    "            \"Longueur_Maximale\": periode_data[\"Longueur_Totale\"].max(),\n",
    "            \"Longueur_Mediane\": periode_data[\"Longueur_Totale\"].median(),\n",
    "            \"Ecart_Type_Longueur\": periode_data[\"Longueur_Totale\"].std(),\n",
    "            \"Pourcentage_Annexes\": (periode_data[\"Presence_Annexes\"].sum() / len(periode_data)) * 100, \n",
    "            \"Nombres d'Annexes\" : periode_data[\"Nombre_Annexes\"].sum()  \n",
    "\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche les résultats globaux et par période\n",
    "longueur = pd.DataFrame([stats_globales_longueur] + periode_longueur)\n",
    "\n",
    "longueur.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enquete_longueur = []\n",
    "\n",
    "for (nom_enquete, periode), couple in data.groupby([\"Nom_d_emprunt\", \"Periode\"]):\n",
    "    if not couple.empty:\n",
    "        enquete_longueur.append({\n",
    "            \"Nom_d_emprunt\": nom_enquete,\n",
    "            \"Periode\": periode,\n",
    "            \"Nombre_de_Posts\": len(couple),\n",
    "            \"Longueur_Moyenne\": couple[\"Longueur_Totale\"].mean(),\n",
    "            \"Longueur_Minimale\": couple[\"Longueur_Totale\"].min(),\n",
    "            \"Longueur_Maximale\": couple[\"Longueur_Totale\"].max(),\n",
    "            \"Ecart_Type_Longueur\": couple[\"Longueur_Totale\"].std(),\n",
    "            \"Longueur_Mediane\": couple[\"Longueur_Totale\"].median(),\n",
    "            \"Pourcentage_Annexes\": (couple[\"Presence_Annexes\"].sum() / len(couple)) * 100,\n",
    "            \"Nombres d'Annexes\": couple[\"Nombre_Annexes\"].sum()\n",
    "        })\n",
    "enquete_longueur = pd.DataFrame(enquete_longueur)\n",
    "enquete_longueur = enquete_longueur.sort_values(by=[\"Nom_d_emprunt\", \"Periode\"])\n",
    "enquete_longueur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longueur_carriere = []\n",
    "for carriere in data[\"type_carriere\"].unique():\n",
    "    carriere_data = data[data[\"type_carriere\"] == carriere].dropna(subset=[\"Longueur_Totale\"])\n",
    "    \n",
    "    if not carriere_data.empty:\n",
    "        longueur_carriere.append({\n",
    "            \"Type\": f\"Carrière: {carriere}\",\n",
    "            \"Nombre_de_Posts\": len(carriere_data),\n",
    "            \"Longueur_Moyenne\": carriere_data[\"Longueur_Totale\"].mean(),\n",
    "            \"Longueur_Minimale\": carriere_data[\"Longueur_Totale\"].min(),\n",
    "            \"Longueur_Maximale\": carriere_data[\"Longueur_Totale\"].max(),\n",
    "            \"Ecart_Type_Longueur\": carriere_data[\"Longueur_Totale\"].std(),\n",
    "            \"Longueur_Mediane\": carriere_data[\"Longueur_Totale\"].median(),\n",
    "            \"Pourcentage_Annexes\": (carriere_data[\"Presence_Annexes\"].sum() / len(carriere_data)) * 100,\n",
    "            \"Nombres d'Annexes\": carriere_data[\"Nombre_Annexes\"].sum()\n",
    "        })\n",
    "\n",
    "longueur_carriere = pd.DataFrame(longueur_carriere)\n",
    "longueur_carriere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentions dans les posts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ici analyser l'usage que nos enquêtés font des mentions (d'auteurs, de maisons d'éditions, de leurs pairs...) à l'aide de @, au sein de leurs posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse pour contenu et annexes\n",
    "for zone, serie in [(\"Contenu\", data[\"Contenu_Post\"]), (\"Annexes\", data[\"Annexes_Post\"])]:\n",
    "    mentions = serie.apply(analyser_mentions)\n",
    "    for key in mentions.iloc[0].keys():  # récupère toutes les clés du dict\n",
    "        data[f\"{key}_{zone}\"] = mentions.apply(lambda x: x[key])\n",
    "\n",
    "# Colonnes totales (booléens)\n",
    "data[\"Contient_Mention\"] = data[\"Contient_Mentions_Contenu\"] | data[\"Contient_Mentions_Annexes\"]\n",
    "data[\"Contient_Mentions_Pairs_Total\"] = data[\"Contient_Mentions_Pairs_Contenu\"] | data[\"Contient_Mentions_Pairs_Annexes\"]\n",
    "data[\"Contient_Mentions_Auteurs_Total\"] = data[\"Contient_Mentions_Auteurs_Contenu\"] | data[\"Contient_Mentions_Auteurs_Annexes\"]\n",
    "data[\"Contient_Mentions_Autres_Total\"] = data[\"Contient_Mentions_Autres_Contenu\"] | data[\"Contient_Mentions_Autres_Annexes\"]\n",
    "data[\"Contient_Mentions_Maisons_Total\"] = data[\"Contient_Mentions_Maisons_Contenu\"] | data[\"Contient_Mentions_Maisons_Annexes\"]\n",
    "\n",
    "# Colonnes totales (nombres)\n",
    "data[\"Nombre_Mentions_Total\"] = data[\"Nombre_Mentions_Contenu\"] + data[\"Nombre_Mentions_Annexes\"]\n",
    "data[\"Mentions_Auteurs_Total\"] = data[\"Mentions_Auteurs_Contenu\"] + data[\"Mentions_Auteurs_Annexes\"]\n",
    "data[\"Mentions_Maisons_Total\"] = data[\"Mentions_Maisons_Contenu\"] + data[\"Mentions_Maisons_Annexes\"]\n",
    "data[\"Mentions_Autres_Total\"] = data[\"Mentions_Autres_Contenu\"] + data[\"Mentions_Autres_Annexes\"]\n",
    "data[\"Mentions_Pairs_Total\"] = data[\"Mentions_Pairs_Contenu\"] + data[\"Mentions_Pairs_Annexes\"]\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrouper par personne et par période pour calculer les statistiques\n",
    "mentions_enquete_period = (\n",
    "    data.groupby([\"Nom_d_emprunt\", \"Periode\"])\n",
    "    .agg(\n",
    "        Nombre_Total_Mentions=(\"Nombre_Mentions_Total\", \"sum\"),\n",
    "        Mentions_Auteurs_Total=(\"Mentions_Auteurs_Total\", \"sum\"),\n",
    "        Mentions_Maisons_Total=(\"Mentions_Maisons_Total\", \"sum\"),\n",
    "        Mentions_Pairs_Total=(\"Mentions_Pairs_Total\", \"sum\"),\n",
    "        Mentions_Autres_Total=(\"Mentions_Autres_Total\", \"sum\"),\n",
    "        Nombre_Mentions_Contenu=(\"Nombre_Mentions_Contenu\", \"sum\"),\n",
    "        Nombre_Mentions_Annexes=(\"Nombre_Mentions_Annexes\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Contenu=(\"Contient_Mentions_Contenu\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Annexes=(\"Contient_Mentions_Annexes\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Total=(\"Contient_Mention\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Pairs=(\"Contient_Mentions_Pairs_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Auteurs=(\"Contient_Mentions_Auteurs_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Autres=(\"Contient_Mentions_Autres_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Maisons=(\"Contient_Mentions_Maisons_Total\", \"sum\"),\n",
    "        Nombre_de_Posts=(\"Date_Post\", \"count\")  # Nombre total de posts\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les pourcentages\n",
    "cols_mentions = [\n",
    "    \"Posts_Contenant_Mentions_Contenu\",\n",
    "    \"Posts_Contenant_Mentions_Annexes\",\n",
    "    \"Posts_Contenant_Mentions_Pairs\",\n",
    "    \"Posts_Contenant_Mentions_Auteurs\",\n",
    "    \"Posts_Contenant_Mentions_Autres\",\n",
    "    \"Posts_Contenant_Mentions_Maisons\"\n",
    "]\n",
    "\n",
    "# Calcul automatique des pourcentages\n",
    "for col in cols_mentions:\n",
    "    mentions_enquete_period[f\"Pourcentage_{col}\"] = (\n",
    "        mentions_enquete_period[col] /\n",
    "        mentions_enquete_period[\"Nombre_de_Posts\"] * 100\n",
    "    )\n",
    "\n",
    "# Cas spécial : total contenu + annexes\n",
    "mentions_enquete_period[\"Pourcentage_Posts_Contenant_Mentions_Total\"] = (\n",
    "    (mentions_enquete_period[\"Posts_Contenant_Mentions_Contenu\"] +\n",
    "     mentions_enquete_period[\"Posts_Contenant_Mentions_Annexes\"]) /\n",
    "    mentions_enquete_period[\"Nombre_de_Posts\"] * 100\n",
    ")\n",
    "mentions_enquete_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_period = (\n",
    "    data.groupby([\"Periode\"])\n",
    "    .agg(\n",
    "        Nombre_Total_Mentions=(\"Nombre_Mentions_Total\", \"sum\"),\n",
    "        Mentions_Auteurs_Total=(\"Mentions_Auteurs_Total\", \"sum\"),\n",
    "        Mentions_Maisons_Total=(\"Mentions_Maisons_Total\", \"sum\"),\n",
    "        Mentions_Pairs_Total=(\"Mentions_Pairs_Total\", \"sum\"),\n",
    "        Mentions_Autres_Total=(\"Mentions_Autres_Total\", \"sum\"),\n",
    "        Nombre_Mentions_Contenu=(\"Nombre_Mentions_Contenu\", \"sum\"),\n",
    "        Nombre_Mentions_Annexes=(\"Nombre_Mentions_Annexes\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Contenu=(\"Contient_Mentions_Contenu\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Annexes=(\"Contient_Mentions_Annexes\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Total=(\"Contient_Mention\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Pairs=(\"Contient_Mentions_Pairs_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Auteurs=(\"Contient_Mentions_Auteurs_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Autres=(\"Contient_Mentions_Autres_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Maisons=(\"Contient_Mentions_Maisons_Total\", \"sum\"),\n",
    "        Nombre_de_Posts=(\"Date_Post\", \"count\")  # Nombre total de posts\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les pourcentages\n",
    "cols_mentions = [\n",
    "    \"Posts_Contenant_Mentions_Contenu\",\n",
    "    \"Posts_Contenant_Mentions_Annexes\",\n",
    "    \"Posts_Contenant_Mentions_Pairs\",\n",
    "    \"Posts_Contenant_Mentions_Auteurs\",\n",
    "    \"Posts_Contenant_Mentions_Autres\",\n",
    "    \"Posts_Contenant_Mentions_Maisons\"\n",
    "]\n",
    "\n",
    "# Calcul automatique des pourcentages\n",
    "for col in cols_mentions:\n",
    "    mentions_period[f\"Pourcentage_{col}\"] = (\n",
    "        mentions_period[col] /\n",
    "        mentions_period[\"Nombre_de_Posts\"] * 100\n",
    "    )\n",
    "\n",
    "# les totaux\n",
    "mentions_period[\"Pourcentage_Posts_Contenant_Mentions_Total\"] = (\n",
    "    (mentions_period[\"Posts_Contenant_Mentions_Contenu\"] +\n",
    "     mentions_period[\"Posts_Contenant_Mentions_Annexes\"]) /\n",
    "    mentions_period[\"Nombre_de_Posts\"] * 100\n",
    ")\n",
    "mentions_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_enquete = (\n",
    "    data.groupby([\"Nom_d_emprunt\"])\n",
    "    .agg(\n",
    "        Nombre_Total_Mentions=(\"Nombre_Mentions_Total\", \"sum\"),\n",
    "        Mentions_Auteurs_Total=(\"Mentions_Auteurs_Total\", \"sum\"),\n",
    "        Mentions_Maisons_Total=(\"Mentions_Maisons_Total\", \"sum\"),\n",
    "        Mentions_Pairs_Total=(\"Mentions_Pairs_Total\", \"sum\"),\n",
    "        Mentions_Autres_Total=(\"Mentions_Autres_Total\", \"sum\"),\n",
    "        Nombre_Mentions_Contenu=(\"Nombre_Mentions_Contenu\", \"sum\"),\n",
    "        Nombre_Mentions_Annexes=(\"Nombre_Mentions_Annexes\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Contenu=(\"Contient_Mentions_Contenu\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Annexes=(\"Contient_Mentions_Annexes\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Total=(\"Contient_Mention\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Pairs=(\"Contient_Mentions_Pairs_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Auteurs=(\"Contient_Mentions_Auteurs_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Autres=(\"Contient_Mentions_Autres_Total\", \"sum\"),\n",
    "        Posts_Contenant_Mentions_Maisons=(\"Contient_Mentions_Maisons_Total\", \"sum\"),\n",
    "        Nombre_de_Posts=(\"Date_Post\", \"count\")  # Nombre total de posts\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "# Calculer les pourcentages\n",
    "cols_mentions = [\n",
    "    \"Posts_Contenant_Mentions_Contenu\",\n",
    "    \"Posts_Contenant_Mentions_Annexes\",\n",
    "    \"Posts_Contenant_Mentions_Pairs\",\n",
    "    \"Posts_Contenant_Mentions_Auteurs\",\n",
    "    \"Posts_Contenant_Mentions_Autres\",\n",
    "    \"Posts_Contenant_Mentions_Maisons\"\n",
    "]\n",
    "\n",
    "# Calcul automatique des pourcentages\n",
    "for col in cols_mentions:\n",
    "    mentions_enquete[f\"Pourcentage_{col}\"] = (\n",
    "        mentions_enquete[col] /\n",
    "        mentions_enquete[\"Nombre_de_Posts\"] * 100\n",
    "    )\n",
    "\n",
    "# les totaux\n",
    "mentions_enquete[\"Pourcentage_Posts_Contenant_Mentions_Total\"] = (\n",
    "    (mentions_enquete[\"Posts_Contenant_Mentions_Contenu\"] +\n",
    "     mentions_enquete[\"Posts_Contenant_Mentions_Annexes\"]) /\n",
    "    mentions_enquete[\"Nombre_de_Posts\"] * 100\n",
    ")\n",
    "mentions_enquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_enquete['type_carriere'] = mentions_enquete['Nom_d_emprunt'].apply(get_type_carriere)\n",
    "# Regroupement par type_carriere avec somme pour les compteurs\n",
    "mentions_carriere = mentions_enquete.groupby(\"type_carriere\").agg({\n",
    "    \"Nombre_Total_Mentions\": \"sum\",\n",
    "    \"Mentions_Auteurs_Total\": \"sum\",\n",
    "    \"Mentions_Maisons_Total\": \"sum\",\n",
    "    \"Mentions_Pairs_Total\": \"sum\",\n",
    "    \"Mentions_Autres_Total\": \"sum\",\n",
    "    \"Nombre_Mentions_Contenu\": \"sum\",\n",
    "    \"Nombre_Mentions_Annexes\": \"sum\",\n",
    "    \"Posts_Contenant_Mentions_Contenu\": \"sum\",\n",
    "    \"Posts_Contenant_Mentions_Annexes\": \"sum\",\n",
    "    \"Posts_Contenant_Mentions_Total\": \"sum\",\n",
    "    \"Posts_Contenant_Mentions_Pairs\": \"sum\",\n",
    "    \"Posts_Contenant_Mentions_Auteurs\": \"sum\",\n",
    "    \"Posts_Contenant_Mentions_Autres\": \"sum\",\n",
    "    \"Posts_Contenant_Mentions_Maisons\": \"sum\",\n",
    "    \"Nombre_de_Posts\": \"sum\",\n",
    "})\n",
    "\n",
    "# Calculer les pourcentages moyens pondérés par nombre de posts (optionnel)\n",
    "# Par exemple : moyenne des pourcentages, pondérée par nombre de posts\n",
    "for col in [\n",
    "    \"Pourcentage_Posts_Contenant_Mentions_Contenu\",\n",
    "    \"Pourcentage_Posts_Contenant_Mentions_Annexes\",\n",
    "    \"Pourcentage_Posts_Contenant_Mentions_Total\"\n",
    "]:\n",
    "    mentions_carriere[col + \"_moyenne_ponderee\"] = (\n",
    "        (mentions_enquete[col] * mentions_enquete[\"Nombre_de_Posts\"]).groupby(mentions_enquete[\"type_carriere\"]).sum()\n",
    "        / mentions_enquete.groupby(\"type_carriere\")[\"Nombre_de_Posts\"].sum()\n",
    "    )\n",
    "\n",
    "mentions_carriere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des statistiques globales\n",
    "stats_globales = {\n",
    "    \"Nombre_Total_Posts\": len(data),\n",
    "    \"Nombre_Total_Mentions\": data[\"Nombre_Mentions_Total\"].sum(),\n",
    "    \"Mentions_Auteurs_Total\": data[\"Mentions_Auteurs_Total\"].sum(),\n",
    "    \"Mentions_Maisons_Total\": data[\"Mentions_Maisons_Total\"].sum(),\n",
    "    \"Mentions_Pairs_Total\": data[\"Mentions_Pairs_Total\"].sum(),\n",
    "    \"Mentions_Autres_Total\": data[\"Mentions_Autres_Total\"].sum(),\n",
    "    \"Pourcentage_Posts_Contenant_Mentions_\": data[\"Contient_Mentions_Contenu\"].sum() / len(data) * 100,\n",
    "    \"Pourcentage_Posts_Contenant_Mentions_Annexes\": data[\"Contient_Mentions_Annexes\"].sum() / len(data) * 100,\n",
    "    \"Pourcentage_Posts_Contenant_Mentions_Total\": (data[\"Contient_Mention\"].sum() / len(data)) * 100,\n",
    "    \"Posts_Contenant_Mentions_Contenu\": data[\"Contient_Mentions_Contenu\"].sum(),\n",
    "    \"Posts_Contenant_Mentions_Annexes\": data[\"Contient_Mentions_Annexes\"].sum(),\n",
    "    \"Posts_Contenant_Mentions_Total\": data[\"Contient_Mention\"].sum(),\n",
    "    \"Posts_Contenant_Mentions_Pairs\": data[\"Contient_Mentions_Pairs_Total\"].sum(),\n",
    "    \"Posts_Contenant_Mentions_Auteurs\": data[\"Contient_Mentions_Auteurs_Total\"].sum(),\n",
    "    \"Posts_Contenant_Mentions_Autres\": data[\"Contient_Mentions_Autres_Total\"].sum(),\n",
    "    \"Posts_Contenant_Mentions_Maisons\": data[\"Contient_Mentions_Maisons_Total\"].sum(),\n",
    "}\n",
    "    \n",
    "\n",
    "# Afficher les statistiques globales\n",
    "for key, value in stats_globales.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag dans les posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse pour contenu et annexes\n",
    "for zone, serie in [(\"Contenu\", data[\"Contenu_Post\"]), (\"Annexes\", data[\"Annexes_Post\"])]:\n",
    "    mentions = serie.apply(analyser_hashtags)\n",
    "    for key in mentions.iloc[0].keys():  # récupère toutes les clés du dict\n",
    "        data[f\"{key}_{zone}\"] = mentions.apply(lambda x: x[key])\n",
    "\n",
    "# Calcul des totaux (hashtags contenu + annexes)\n",
    "data[\"Nombre_Hashtags_Total\"] = data[\"Nombre_Hashtags_Contenu\"] + data[\"Nombre_Hashtags_Annexes\"]\n",
    "data[\"Contient_Hashtags\"] = data[\"Contient_Hashtags_Contenu\"] | data[\"Contient_Hashtags_Annexes\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des hastags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrouper par personne et par période pour calculer les statistiques\n",
    "hashtags_enquete_periode = (\n",
    "    data.groupby([\"Nom_d_emprunt\", \"Periode\"])\n",
    "    .agg(\n",
    "        Nombre_Total_Hashtags=(\"Nombre_Hashtags_Total\", \"sum\"),\n",
    "        Nombre_Hashtags_Contenu=(\"Nombre_Hashtags_Contenu\", \"sum\"),\n",
    "        Nombre_Hashtags_Annexes=(\"Nombre_Hashtags_Annexes\", \"sum\"),\n",
    "        Nombre_de_Posts=(\"Date_Post\", \"count\")  # Nombre total de posts\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculer le pourcentage de posts contenant des hashtags\n",
    "hashtags_enquete_periode[\"Pourcentage_Posts_Contenant_Hashtags_Contenu\"] = (\n",
    "    (data[\"Nombre_Hashtags_Contenu\"] > 0).groupby([data[\"Nom_d_emprunt\"], data[\"Periode\"]]).sum() /\n",
    "    hashtags_enquete_periode.set_index([\"Nom_d_emprunt\", \"Periode\"])[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "hashtags_enquete_periode[\"Pourcentage_Posts_Contenant_Hashtags_Annexes\"] = (\n",
    "    (data[\"Nombre_Hashtags_Annexes\"] > 0).groupby([data[\"Nom_d_emprunt\"], data[\"Periode\"]]).sum() /\n",
    "    hashtags_enquete_periode.set_index([\"Nom_d_emprunt\", \"Periode\"])[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "hashtags_enquete_periode[\"Pourcentage_Posts_Contenant_Hashtags_Total\"] = (\n",
    "    ((data[\"Nombre_Hashtags_Contenu\"] > 0) | (data[\"Nombre_Hashtags_Annexes\"] > 0)).groupby([data[\"Nom_d_emprunt\"], data[\"Periode\"]]).sum() /\n",
    "    hashtags_enquete_periode.set_index([\"Nom_d_emprunt\", \"Periode\"])[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "\n",
    "hashtags_enquete_periode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_enquete = (\n",
    "    data.groupby(\"Nom_d_emprunt\")\n",
    "    .agg(\n",
    "        Nombre_Total_Hashtags=(\"Nombre_Hashtags_Total\", \"sum\"),\n",
    "        Nombre_Hashtags_Contenu=(\"Nombre_Hashtags_Contenu\", \"sum\"),\n",
    "        Nombre_Hashtags_Annexes=(\"Nombre_Hashtags_Annexes\", \"sum\"),\n",
    "        Nombre_de_Posts=(\"Date_Post\", \"count\")  # Total de posts par personne\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calcul du pourcentage de posts avec hashtags pour chaque enquêté\n",
    "hashtags_enquete[\"Pourcentage_Posts_Contenant_Hashtags_Contenu\"] = (\n",
    "    (data[\"Nombre_Hashtags_Contenu\"] > 0).groupby(data[\"Nom_d_emprunt\"]).sum() /\n",
    "    hashtags_enquete.set_index(\"Nom_d_emprunt\")[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "hashtags_enquete[\"Pourcentage_Posts_Contenant_Hashtags_Annexes\"] = (\n",
    "    (data[\"Nombre_Hashtags_Annexes\"] > 0).groupby(data[\"Nom_d_emprunt\"]).sum() /\n",
    "    hashtags_enquete.set_index(\"Nom_d_emprunt\")[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "hashtags_enquete[\"Pourcentage_Posts_Contenant_Hashtags_Total\"] = (\n",
    "    ((data[\"Nombre_Hashtags_Contenu\"] > 0) | (data[\"Nombre_Hashtags_Annexes\"] > 0))\n",
    "    .groupby(data[\"Nom_d_emprunt\"]).sum() /\n",
    "    hashtags_enquete.set_index(\"Nom_d_emprunt\")[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "\n",
    "hashtags_enquete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_enquete['type_carriere'] = hashtags_enquete['Nom_d_emprunt'].apply(get_type_carriere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des hastags par type de carrière\n",
    "hashtags_carriere = (\n",
    "    hashtags_enquete\n",
    "    .groupby(\"type_carriere\")\n",
    "    .agg(\n",
    "        Nombre_Total_Hashtags=(\"Nombre_Total_Hashtags\", \"sum\"),\n",
    "        Nombre_Hashtags_Contenu=(\"Nombre_Hashtags_Contenu\", \"sum\"),\n",
    "        Nombre_Hashtags_Annexes=(\"Nombre_Hashtags_Annexes\", \"sum\"),\n",
    "        Nombre_de_Posts=(\"Nombre_de_Posts\", \"sum\"),\n",
    "        Pourcentage_Moyen_Posts_Contenant_Hashtags_Contenu=(\"Pourcentage_Posts_Contenant_Hashtags_Contenu\", \"mean\"),\n",
    "        Pourcentage_Moyen_Posts_Contenant_Hashtags_Annexes=(\"Pourcentage_Posts_Contenant_Hashtags_Annexes\", \"mean\"),\n",
    "        Pourcentage_Moyen_Posts_Contenant_Hashtags_Total=(\"Pourcentage_Posts_Contenant_Hashtags_Total\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "hashtags_carriere = hashtags_carriere.set_index(\"type_carriere\")\n",
    "hashtags_carriere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des hashtags par période\n",
    "hashtags_periode = (\n",
    "    data.groupby([ \"Periode\"])\n",
    "    .agg(\n",
    "        Nombre_Total_Hashtags=(\"Nombre_Hashtags_Total\", \"sum\"),\n",
    "        Nombre_Hashtags_Contenu=(\"Nombre_Hashtags_Contenu\", \"sum\"),\n",
    "        Nombre_Hashtags_Annexes=(\"Nombre_Hashtags_Annexes\", \"sum\"),\n",
    "        Nombre_de_Posts=(\"Date_Post\", \"count\")  \n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculer le pourcentage de posts contenant des hashtags\n",
    "hashtags_periode[\"Pourcentage_Posts_Contenant_Hashtags_Contenu\"] = (\n",
    "    (data[\"Nombre_Hashtags_Contenu\"] > 0).groupby([data[\"Periode\"]]).sum() /\n",
    "    hashtags_periode.set_index([\"Periode\"])[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "hashtags_periode[\"Pourcentage_Posts_Contenant_Hashtags_Annexes\"] = (\n",
    "    (data[\"Nombre_Hashtags_Annexes\"] > 0).groupby([data[\"Periode\"]]).sum() /\n",
    "    hashtags_periode.set_index([\"Periode\"])[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "hashtags_periode[\"Pourcentage_Posts_Contenant_Hashtags_Total\"] = (\n",
    "    ((data[\"Nombre_Hashtags_Contenu\"] > 0) | (data[\"Nombre_Hashtags_Annexes\"] > 0)).groupby([data[\"Periode\"]]).sum() /\n",
    "    hashtags_periode.set_index([\"Periode\"])[\"Nombre_de_Posts\"] * 100\n",
    ").values\n",
    "\n",
    "hashtags_periode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des statistiques globales à propos des hashtags\n",
    "hashtags_globales = {\n",
    "    \"Nombre_Total_Posts\": len(data),\n",
    "    \"Nombre_Total_Hashtags\": data[\"Nombre_Hashtags_Total\"].sum(),\n",
    "    \"Nombre_Hashtags_Contenu\": data[\"Nombre_Hashtags_Contenu\"].sum(),\n",
    "    \"Nombre_Hashtags_Annexes\": data[\"Nombre_Hashtags_Annexes\"].sum(),\n",
    "    \"Nombre_Hashtags_Total\": data[\"Nombre_Hashtags_Total\"].sum(),\n",
    "    \"Pourcentage_Posts_Contenant_Hashtags_Contenu\": data[\"Contient_Hashtags_Contenu\"].sum() / len(data) * 100,\n",
    "    \"Pourcentage_Posts_Contenant_Hashtags_Annexes\": data[\"Contient_Hashtags_Annexes\"].sum() / len(data) * 100,\n",
    "    \"Pourcentage_Posts_Contenant_Hashtags_Total\": (data[\"Contient_Hashtags\"].sum() / len(data)) * 100,\n",
    "    \"Posts_Contenant_Hashtags_Contenu\": data[\"Contient_Hashtags_Contenu\"].sum(),\n",
    "    \"Posts_Contenant_Hashtags_Annexes\": data[\"Contient_Hashtags_Annexes\"].sum(),   \n",
    "}\n",
    "    \n",
    "for key, value in hashtags_globales.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses des images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes binaires\n",
    "colonnes_binaires = ['Plusieurs_Images', 'Personne', 'PostProduction', 'Mots']\n",
    "images_globales = {}\n",
    "\n",
    "for col in colonnes_binaires:\n",
    "    images_globales[col] = {\n",
    "        'Présence': data[col].sum(),\n",
    "        'Absence': data[col].count() - data[col].sum(),\n",
    "        'Pourcentage_Présence': data[col].mean() * 100,\n",
    "        'Total': data[col].count()\n",
    "    }\n",
    "\n",
    "images_globales = pd.DataFrame(images_globales)\n",
    "images_globales\n",
    "\n",
    "# lister les posts sur lesquels il n'y a pas ces informations pour vérifier les erreurs potentielles\n",
    "#posts_sans_info = data[data[colonnes_binaires].isnull().any(axis=1)][['ID_Post', 'Nom_d_emprunt', 'Periode', 'Date_Post'] + colonnes_binaires]\n",
    "#posts_sans_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par personne des images\n",
    "images_enquete = data.groupby('Nom_d_emprunt')[colonnes_binaires].sum()\n",
    "images_enquete['Total_Posts'] = data.groupby('Nom_d_emprunt').size()\n",
    "images_enquete = images_enquete.div(images_enquete['Total_Posts'], axis=0) * 100\n",
    "\n",
    "images_enquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enlever les posts dont les images ne sont plus disponibles lors de la collecte des données\n",
    "colonnes_image = [\"Personne\", \"PostProduction\", \"Mots\", \"Plusieurs_Images\"]\n",
    "data_valable = data.dropna(subset=colonnes_image, how='all')\n",
    "\n",
    "# Calculer les totaux des colonnes binaires, par période, sur ces données filtrées\n",
    "images_periode = data_valable.groupby('Periode')[colonnes_binaires].sum()\n",
    "images_periode['Total_Posts'] = data.groupby('Periode').size()\n",
    "\n",
    "images_periode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par carrière des images\n",
    "images_carriere = data.groupby('type_carriere')[colonnes_binaires].sum()\n",
    "images_carriere['Total_Posts'] = data.groupby('type_carriere').size()\n",
    "images_carriere = images_carriere.div(images_carriere['Total_Posts'], axis=0) * 100\n",
    "\n",
    "images_carriere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée par personne et période\n",
    "images_enquete_periode = data.groupby(['Nom_d_emprunt', 'Periode'])[colonnes_binaires].sum()\n",
    "images_enquete_periode['Total_Posts'] = data.groupby(['Nom_d_emprunt', 'Periode']).size()\n",
    "images_enquete_periode = images_enquete_periode.div(images_enquete_periode['Total_Posts'], axis=0) * 100\n",
    "images_enquete_periode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la nature des posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse globale\n",
    "nature_globales = data[['N_Ecrit', 'N_Video', 'N_Audio']].sum().to_frame(name='Total')\n",
    "nature_globales['Pourcentage'] = (nature_globales['Total'] /596) * 100\n",
    "nature_globales['Total_Posts'] = len(data)\n",
    "\n",
    "nature_globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par enquete\n",
    "nature_enquete = data.groupby('Nom_d_emprunt')[['N_Ecrit', 'N_Video', 'N_Audio']].sum()\n",
    "nature_enquete['Total_Posts'] = data.groupby('Nom_d_emprunt').size()\n",
    "nature_enquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par période\n",
    "nature_periode = data.groupby('Periode')[['N_Ecrit', 'N_Video', 'N_Audio']].sum()\n",
    "nature_periode['Total_Posts'] = data.groupby('Periode').size()\n",
    "nature_periode = nature_periode.div(nature_periode['Total_Posts'], axis=0) * 100\n",
    "nature_periode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée par personne et période\n",
    "nature_enquete_periode = data.groupby(['Nom_d_emprunt', 'Periode'])[['N_Ecrit', 'N_Video', 'N_Audio']].sum()\n",
    "nature_enquete_periode['Total_Posts'] = data.groupby(['Nom_d_emprunt', 'Periode']).size()\n",
    "nature_enquete_periode = nature_enquete_periode.div(nature_enquete_periode['Total_Posts'], axis=0) * 100\n",
    "nature_enquete_periode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par carriere\n",
    "nature_carriere = data.groupby('type_carriere')[['N_Ecrit', 'N_Video', 'N_Audio']].sum()\n",
    "nature_carriere['Total_Posts'] = data.groupby('type_carriere').size()\n",
    "nature_carriere = nature_carriere.div(nature_carriere['Total_Posts'], axis=0) * 100\n",
    "\n",
    "nature_carriere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tableaux pour le clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "supra_df = frequence.merge(enquete_longueur, on=[\"Nom_d_emprunt\", \"Periode\"], how=\"left\")\n",
    "supra_df = supra_df.merge(mentions_enquete_period.drop(columns = \"Nombre_de_Posts\"), on=[\"Nom_d_emprunt\", \"Periode\"], how=\"left\")\n",
    "supra_df = supra_df.merge(hashtags_enquete_periode, on=[\"Nom_d_emprunt\", \"Periode\"], how=\"left\")\n",
    "supra_df = supra_df.merge(images_enquete_periode, on=[\"Nom_d_emprunt\", \"Periode\"], how=\"left\")\n",
    "supra_df = supra_df.merge(nature_enquete_periode, on=[\"Nom_d_emprunt\", \"Periode\"], how=\"left\")\n",
    "\n",
    "supra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Afficher la liste des noms de colonnes de merged_df\n",
    "supra_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On visualise les colonnes de notre df et on enleve celles qui ne sont pas nécessaires pour le clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supra_df.drop(columns=[ 'Carrière',\n",
    " 'Intervalle_Mediane',\n",
    " 'Longueur_Minimale',\n",
    " 'Longueur_Maximale',\n",
    " 'Ecart_Type_Longueur',\n",
    " 'Longueur_Mediane',\n",
    " \"Nombres d'Annexes\",\n",
    "  'Intervalle_Minimum',\n",
    " 'Intervalle_Maximum','Total_Posts_y','Total_Posts_x', 'Pourcentage_Posts_Contenant_Hashtags_Contenu',\n",
    " 'Pourcentage_Posts_Contenant_Hashtags_Annexes', 'Nombre_Hashtags_Contenu',\n",
    " 'Nombre_Hashtags_Annexes', 'Pourcentage_Posts_Contenant_Mentions_Contenu',\n",
    " 'Pourcentage_Posts_Contenant_Mentions_Annexes', 'Nombre_Mentions_Contenu',\n",
    " 'Nombre_Mentions_Annexes',\n",
    " 'Posts_Contenant_Mentions_Contenu',\n",
    " 'Posts_Contenant_Mentions_Annexes',\n",
    " 'Nombre_de_Posts_y','Nombre_de_Posts_x',  'Mentions_Auteurs_Total',\n",
    " 'Mentions_Maisons_Total',\n",
    " 'Mentions_Autres_Total',\n",
    " 'Nombre_Mentions_Contenu',\n",
    " 'Nombre_Mentions_Annexes','Posts_Contenant_Mentions_Total',\n",
    " 'Posts_Contenant_Mentions_Pairs',\n",
    " 'Posts_Contenant_Mentions_Auteurs',\n",
    " 'Posts_Contenant_Mentions_Autres',\n",
    " 'Posts_Contenant_Mentions_Maisons','Pourcentage_Posts_Contenant_Mentions_Autres','Mentions_Pairs_Total'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Sauvegarder le DataFrame final pour le clustering\n",
    "#merged_df.to_excel(\"Clustering_Data.xlsx\", index=False)\n",
    "supra_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "supra_df\n",
    "# On obtient le DataFrame final avec les colonnes nécessaires pour le clustering des couples enquêtés et périodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Pour obtenir le dataframe nécesaire au clustering sur les enquetés, on va faire des sommes et des moyennes pondérées sur les indicateurs statistiques. \n",
    "df = supra_df.copy()\n",
    "\n",
    "colonnes_a_sommer = [  'Nombre_de_Posts', 'Nombre_Total_Hashtags', 'Nombre_Total_Mentions',\n",
    "  ]\n",
    "\n",
    "colonnes_a_pond_moy = [\n",
    "'Plusieurs_Images',\n",
    " 'Personne',\n",
    " 'PostProduction',\n",
    " 'Mots',\n",
    " 'N_Ecrit',\n",
    " 'N_Video',\n",
    " 'N_Audio',  'Pourcentage_Posts_Contenant_Hashtags_Total', 'Pourcentage_Posts_Contenant_Mentions_Total',  'Longueur_Moyenne',\n",
    " 'Pourcentage_Annexes',\n",
    " 'Intervalle_Moyen',\n",
    " 'Ecart_Type_Intervalle', \n",
    " 'Pourcentage_Posts_Contenant_Mentions_Pairs',\n",
    " 'Pourcentage_Posts_Contenant_Mentions_Auteurs',\n",
    " 'Pourcentage_Posts_Contenant_Mentions_Maisons',\n",
    "]\n",
    "\n",
    "somme = df.groupby(\"Nom_d_emprunt\")[colonnes_a_sommer].sum().reset_index()\n",
    "\n",
    "for col in colonnes_a_pond_moy:\n",
    "    df[col + \"_pond\"] = df[col] * df[\"Nombre_de_Posts\"]\n",
    "\n",
    "ponds = (\n",
    "    df.groupby(\"Nom_d_emprunt\")[[col + \"_pond\" for col in colonnes_a_pond_moy] + [\"Nombre_de_Posts\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "for col in colonnes_a_pond_moy:\n",
    "    ponds[col] = ponds[col + \"_pond\"] / ponds[\"Nombre_de_Posts\"]\n",
    "\n",
    "ponds = ponds[[\"Nom_d_emprunt\"] + colonnes_a_pond_moy]\n",
    "\n",
    "df_global = pd.merge(somme, ponds, on=\"Nom_d_emprunt\")\n",
    "\n",
    "df_global.head()\n",
    "#df_global.to_excel(\"Clustering_Global.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
